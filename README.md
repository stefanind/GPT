This is a complete adaptation of GPT-2 (with some GPT-3 hyperparameters) followed from Andrej Karpathy's YouTube lectures, "Neural Networks: Zero to Hero". Specifically, this is a follow along from the video "Let's reproduce GPT-2." 

The code is completely his (I take no rights to it), but I rewrote it by following along and adding my own comments for educational purposes. Therefore, I understand every piece of code written and the role it plays.

Overtime, I will make my own additions to it. 

Currently, I am at the process initializing training. I will be creating an instance through Lambda labs for training. I have not done it yet because I truly don't have the money. It will take approximately 3-4 hours of training usin

