This is a complete adaptation of GPT-2 (with some GPT-3 hyperparameters) followed from Andrej Karpathy's YouTube lectures, "Neural Networks: Zero to Hero". Specifically, this is a follow along from the video "Let's reproduce GPT-2." 

The code is completely his (I take no rights to it), but I rewrote it by following along and adding my own comments for educational purposes. Therefore, I understand every piece of code written and the role it plays. I also heavily modularized the code.

Overtime, I will make my own additions to it and will document them appropriately.

Currently, I am at the process training. I will be creating an instance through Lambda labs to utilize their resources. Distributed training is set up such that one GPU or multiple can be used. 

